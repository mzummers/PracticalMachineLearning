---
title: "Practical Machine Learning - Class Project"
author: "MZ"
date: "February 15, 2017"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary
Data from accelerometers located on belt, forearm, arm, and dumbell of 6 participants is used to create a machine learning algorithm to predict workout efficiency. The random forest method was selected as the best of 3 modeling methods. The algorithm is validated using the validation data set. Prediction is tested in the course quiz correctly (all 20 questions were answered correctly).


## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit allows to collect a large amount of data about personal activity relatively inexpensively. These types of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it (Ugulino et al, 2012). In this project, data from accelerometers located on the belt, forearm, arm, and dumbell of 6 participants is used to create a machine learning algorithm to predict workout efficiency. The algorithm is validated using the second data set. Prediction was tested in the course quiz with 100% accuracy.

## Analysis
The analysis was completed in 3 steps.

1. Preparing Data for Analysis.<br>
The data was downloaded from the Coursera-Practical Machine Learning website into into R. Data was then sufficiently cleaned (e.g. missing data converted to NA). Non-essential data (such as meta data of the participant, time of workout) was removed from consideration. Finally the data set was partitioned into a training and validation data set.

2. Developing Algorithm.<br>
First step was to determine the model that would provide the highest accuracy. Three models were compared
CART model
Stochastic Gradient Boosting
Random Forests
Code and model summary can be found in the appendix. In summary, random forests was selected as it indicated hightest accuracy.
Developing the prediction model using training data and the random forest method indicated high accuracy. this was confirmed using the prediction function on the training set.
The same process was then used on the validation and complete trainig set. 

3. Validating algorithm. 
Final step was to apply the model to the training data set to predict 20 outcomes correctly. The matrix is shown below and was validated by entering the data into the final quiz on the Coursera website.

## Getting & Cleaning Data

Standard set of packages were loaded including package caret, used for the analysis. Most libraries are loaded as a default and may not be used for this project.
```{loading libraries, message = FALSE, warning = FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(AppliedPredictiveModeling)
library(lattice)
library(caret)
library (Hmisc) ## required for cut2 function
library(rpart)
library(rattle)
library(randomForest)
library(gbm)
library(e1071)
library(DT)
```
<br>
Data were downloaded from the Coursera-Practical Machine Learning course and read into data frames. Cursory review identified many NA / blank values. Hence all missing data were converted into NA's
```{loading data, message = FALSE, warning = FALSE}
myDir <- "~/Z_R/8) Practical Machine Learning/Class Project/"
setwd(myDir)
## getwd()
## list.files()

## Reading data sets. Note: any non-numeric value will be set to NA
train<-  read.csv("pml-training.csv", sep=",", header=TRUE, na.strings = c("NA","",'#DIV/0!'))
Number.Rows.train<- nrow(train)
Number.Columns.train<- ncol(train)

testing<-  read.csv("pml-testing.csv", sep=",", header=TRUE, na.strings = c("NA","",'#DIV/0!'))
Number.Rows.testing<- nrow(testing)
Number.Columns.testing<- ncol(testing)

## identical(train, testing) ## checking if data sets are differnt or not
```
<br>
Review of data revealed that several data columns consisted of mostly NA values. These columns were removed from the data frames.

Non-essential data, such as participant data, time readings, were removed as well
```{cleaning partitioning data, message = FALSE, warning = FALSE}

## Cleaning the trainig set, removing columns with N/A's
train.clean <- train[,(colSums(is.na(train)) == 0)]
testing.clean <- testing[,(colSums(is.na(testing)) == 0)]

## Remove non-essential data (e.g. time stamps, names etc., first 7 columns for train)
train.clean<- train.clean[,-(1:7)]
testing.clean<- testing.clean[,-(1:6)]

## create Data Set for Model Building
set.seed(12345)
inTrain <- createDataPartition(y=train.clean$classe, p=0.7, list=F)
train.clean.subset <- train.clean[inTrain, ]
validation.clean.subset <- train.clean[-inTrain, ]
```
<br>
As the appendix will show, random forests seems to be the best method for the analysis. Using the claret package for the analysis, all remaining data were included in the data model, with the dependent variable 'classe' being a factor variable.

```{fitting on train data, results = "hide", message=FALSE, warning = FALSE}

## fitting model using train data, method = random forests
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
fit <- train(classe ~ ., data=train.clean.subset, method="rf", trControl=fitControl)
fit.train.subset<- fit$finalModel
fit.train.subset
```

```{output for initial model, results = "hide", message=FALSE, warning = FALSE}
##      Call:
## randomForest(x = x, y = y, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2

##         OOB estimate of  error rate: 0.67%
## Confusion matrix:
##    A    B    C    D    E class.error
##A 3902    2    1    1    0 0.001024066
##B   14 2634   10    0    0 0.009029345
##C    0   20 2375    1    0 0.008764608
##D    0    0   35 2215    2 0.016429840
##E    0    0    0    6 2519 0.002376238

```
<br>
The model then was used for prediction, using the validation portion of the train data.

```{predicting train, results = "hide", message=FALSE, warning = FALSE}
predict.validations <- predict(fit, newdata=validation.clean.subset)
predict.training<- confusionMatrix(validation.clean.subset$classe, predict.validations)
predict.training$overall
```
<br>
As expected, model accuracy was high.
```{output train prediction, results = "hide", message=FALSE, warning = FALSE}
#     Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull AccuracyPValue 
#     0.9884452      0.9853801      0.9853742      0.9910164      0.2864911      0.0000000
```
<br>
For information purposes only the same methodoly was applied to the entire data set. This might be unusual, however was a useful step in practicing the methodology. Findings were, no surprise, confirmed.
```{fitting full train , results = "hide", message=FALSE, warning = FALSE}
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
fit <- train(classe ~ ., data=train.clean, method="rf", trControl=fitControl)
fit.train.full<- fit$finalModel
fit.train.full
```


```{output full train, results = "hide", message=FALSE, warning = FALSE}
# Call:
#  randomForest(x = x, y = y, mtry = param$mtry) 
#               Type of random forest: classification
#                     Number of trees: 500
#No. of variables tried at each split: 27
#
#        OOB estimate of  error rate: 0.44%
#Confusion matrix:
#     A    B    C    D    E  class.error
#A 5575    3    1    0    1 0.0008960573
#B   19 3773    4    1    0 0.0063207796
#C    0   11 3401   10    0 0.0061367621
#D    0    0   23 3190    3 0.0080845771
#E    0    1    4    5 3597 0.0027723870
```
<br> 

```{predicting full, results = "hide", message=FALSE, warning = FALSE}
## predicting model using full training data set
predict.validations.training <- predict(fit, newdata=train.clean)
predict.training.all<- confusionMatrix(validation.clean.subset$classe, predict.validations)
predict.training.all$overall
```

```{output predicting full train, results = "hide", message=FALSE, warning = FALSE}
#      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull #AccuracyPValue 
#     0.9884452      0.9853801      0.9853742      0.9910164      0.2864911      #0.0000000 
```
<br>

To test our model, we applied it to the testing data set (20 observations with 54 variables). The results were printed below, entered into the project quiz, and 100% validated.
```{predicting quiz, results = "hide", message=FALSE, warning = FALSE}
class.project.prediction <- predict(fit, newdata=testing.clean)
prediction.results <- data.frame(question=testing.clean$problem_id,
      prediction=class.project.prediction)
print(prediction.results)
```

```{output predicting quiz, results = "hide", message=FALSE, warning = FALSE}
#      question  prediction
# 1           1         B
# 2           2         A
# 3           3         B
# 4           4         A
# 5           5         A
# 6           6         E
# 7           7         D
# 8           8         B
# 9           9         A
# 10         10         A
# 11         11         B
# 12         12         C
# 13         13         B
# 14         14         A
# 15         15         E
# 16         16         E
# 17         17         A
# 18         18         B
# 19         19         B
# 20         20         B
```
This concludes the project report.On a side node, I did experience difficulties with knitr as the train function was running for about 3-4 minutes on my Dell laptop with Core i5 processor.


## Appendix
A. CART Model
```{appendix 1 code , results = "hide", message=FALSE, warning = FALSE}
model_cart <- train(classe ~ ., data=train.clean.subset,trControl=fitControl, method='rpart')
model_cart
```

```{appendix 1 output, results = "hide", message=FALSE, warning = FALSE}
# CART 
# 
# 13737 samples
#    52 predictor
#     5 classes: 'A', 'B', 'C', 'D', 'E' 
# 
# No pre-processing
# Resampling: Cross-Validated (3 fold) 
# Summary of sample sizes: 9157, 9159, 9158 
# Resampling results across tuning parameters:
# 
#   cp          Accuracy   Kappa    
#   0.03753433  0.5166387  0.3761371
#   0.05987862  0.4498701  0.2653175
#   0.11585800  0.3107723  0.0404553
# 
# Accuracy was used to select the optimal model using  the largest value.
# The final value used for the model was cp = 0.03753433. 
```

B. Stochastic Gradient Boosting 
```{appendix 2 code , results = "hide", message=FALSE, warning = FALSE}
model_gbm <- train(classe ~ ., data=train.clean.subset,trControl=fitControl,method='gbm')
model_gbm
```

```{appendix 2 output, results = "hide", message=FALSE, warning = FALSE}
# Stochastic Gradient Boosting 
# 
# 13737 samples
#    52 predictor
#     5 classes: 'A', 'B', 'C', 'D', 'E' 
# 
# No pre-processing
# Resampling: Cross-Validated (3 fold) 
# Summary of sample sizes: 9159, 9157, 9158 
# Resampling results across tuning parameters:
# 
#   interaction.depth  n.trees  Accuracy   Kappa    
#   1                   50      0.7522758  0.6858878
#   1                  100      0.8212852  0.7736615
#   1                  150      0.8577558  0.8199343
#   2                   50      0.8549902  0.8162241
#   2                  100      0.9074037  0.8828054
#   2                  150      0.9328092  0.9149739
#   3                   50      0.8957561  0.8679864
#   3                  100      0.9405982  0.9248242
#   3                  150      0.9596710  0.9489738
# 
# Tuning parameter 'shrinkage' was held constant at a value of 0.1
# Tuning
#  parameter 'n.minobsinnode' was held constant at a value of 10
# Accuracy was used to select the optimal model using  the largest value.
# The final values used for the model were n.trees = 150, interaction.depth = 3, shrinkage
#  = 0.1 and n.minobsinnode = 10. 
```


C. Random Forest Method
```{appendix 3 code , results = "hide", message=FALSE, warning = FALSE}
model_rf <- train(classe ~ ., data=train.clean.subset, trControl=fitControl, method='rf', ntree=100)
model_rf
```

```{appendix 3 output, results = "hide", message=FALSE, warning = FALSE}
# Random Forest 
# 
# 13737 samples
#    52 predictor
#     5 classes: 'A', 'B', 'C', 'D', 'E' 
# 
# No pre-processing
# Resampling: Cross-Validated (3 fold) 
# Summary of sample sizes: 9157, 9159, 9158 
# Resampling results across tuning parameters:
# 
#   mtry  Accuracy   Kappa    
#    2    0.9871151  0.9836981
#   27    0.9881343  0.9849890
#   52    0.9839848  0.9797408
# 
# Accuracy was used to select the optimal model using  the largest value.
# The final value used for the model was mtry = 27. 
```

<br><br> 

## Citations: 

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 
